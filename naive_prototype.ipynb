{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "col_type = {'anaemia':np.bool, 'diabetes':np.bool, \n",
    "            'high_blood_pressure':np.str, 'sex':np.bool, \n",
    "            'smoking':np.bool, 'DEATH_EVENT':np.str}\n",
    "\n",
    "df = df.astype(col_type)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(\"DEATH_EVENT\", axis=1), \n",
    "                                                    df['DEATH_EVENT'], train_size=0.67, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_prob_categorical(nums:pd.core.series.Series):\n",
    "    return nums.value_counts(normalize=True)\n",
    "    \n",
    "def get_std_dev(nums: pd.core.series.Series):\n",
    "    mean = nums.mean()\n",
    "    return (((nums - mean)**2).aggregate(np.sum)/(len(nums)-1))**0.5\n",
    "# prior_prob_categorical(df['sex'])[\"1\"]\n",
    "\n",
    "def calculate_prob(x, mean, std):\n",
    "    exponent = math.exp((-1 * (x - mean)**2)/(2*(std**2)))\n",
    "    return (1 / (std * (2*math.pi)**0.5)) * exponent\n",
    "\n",
    "assert(calculate_prob(1, 1, 1) == 0.3989422804014327)\n",
    "assert(calculate_prob(2, 1, 1) == 0.24197072451914337)\n",
    "\n",
    "def train_naive_bayes(x_train, y_train):\n",
    "    # dictionary that stores all probabiltiies for categorical or mean/std for continuous data\n",
    "    prob = dict()\n",
    "    col_y = y_train.name\n",
    "    classes = np.unique(y_train)\n",
    "    df = x_train.copy()\n",
    "    df[col_y] = y_train\n",
    "    \n",
    "    class_counts = y_train.value_counts(normalize=True)\n",
    "    prob['classes'] = {cls: class_counts[cls] for cls in classes}\n",
    "    \n",
    "    # go through each class and calculate piori probabilities for each unique column value\n",
    "    for cls in classes:\n",
    "        tmp = df[df[col_y] == cls]\n",
    "\n",
    "        for col in x_train.columns:\n",
    "            col = str(col)\n",
    "            if col != col_y:\n",
    "                if col not in prob.keys():\n",
    "                    prob[col] = dict()\n",
    "                \n",
    "                prob[col][cls] = dict()\n",
    "\n",
    "                if x_train.dtypes[col] == object:\n",
    "                    val_probs = prior_prob_categorical(tmp[col])\n",
    "                    for val in np.unique(tmp[col]):\n",
    "                        try:\n",
    "                            prob[col][cls][val] = val_probs[val]\n",
    "                        \n",
    "                        except IndexError:\n",
    "                            prob[col][cls][val] = 0\n",
    "\n",
    "                else:\n",
    "                    for cls_2 in classes:\n",
    "                        prob[col][cls_2] = {\n",
    "                            'mean': tmp[col].mean(),\n",
    "                            'std': get_std_dev(tmp[col])}\n",
    "\n",
    "    return prob\n",
    "\n",
    "def predict_naive_bayes(x_test, prob):\n",
    "    classes = list(prob[list(prob.keys())[0]].keys())\n",
    "    y_pred = list()\n",
    "    \n",
    "    # calculate posteri probabilities for each row in test dataset\n",
    "    for i in range(len(x_test)):\n",
    "        row = x_test.iloc[i]\n",
    "        cls_prob = list()\n",
    "        for cls in classes:\n",
    "#             cls = str(cls)\n",
    "            tmp_prob = prob['classes'][cls]\n",
    "            for col in x_test.columns:\n",
    "                try:\n",
    "                    # for categorical data\n",
    "                    tmp_prob *= prob[col][cls][row[col]]\n",
    "                \n",
    "                # for continuous data\n",
    "                except KeyError:\n",
    "                    tmp_prob *= calculate_prob(row[col], prob[col][cls]['mean'], prob[col][cls]['std'])\n",
    "\n",
    "            cls_prob += [(cls, tmp_prob)]\n",
    "\n",
    "        # normalize\n",
    "        s = 0\n",
    "        for c in cls_prob:\n",
    "            s += c[1]\n",
    "            \n",
    "#         print([c[1]/s for c in cls_prob])\n",
    "\n",
    "        y_pred += [max(cls_prob, key=lambda x: x[1])[0]]\n",
    "        \n",
    "    return np.array(y_pred)\n",
    "\n",
    "def nb_predict(x_train, x_test, y_train, y_test):\n",
    "    clf = CategoricalNB()\n",
    "    clf.fit(x_train[categorical], y_train)\n",
    "    score_clf = clf.score(x_test[categorical], y_test)\n",
    "    \n",
    "    prob_categorical = train_naive_bayes(x_train[categorical], y_train)\n",
    "    score_categorical = accuracy_score(y_test, predict_naive_bayes(x_test[categorical], prob_categorical))\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train[numerical], y_train)\n",
    "    score_gnb = gnb.score(x_test[numerical], y_test)\n",
    "\n",
    "    prob_numerical = train_naive_bayes(x_train[numerical], y_train)\n",
    "    score_numerical = accuracy_score(y_test, predict_naive_bayes(x_test[numerical], prob_numerical))\n",
    "\n",
    "#     print('Naive Bayes:', score_categorical, score_clf, score_numerical, score_gnb)\n",
    "    print('Naive Bayes:', score_categorical, score_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.7333333333333333 0.7333333333333333\n",
      "Naive Bayes: 0.8 0.8\n",
      "Naive Bayes: 0.6 0.6\n",
      "Naive Bayes: 0.6666666666666666 0.6666666666666666\n",
      "Naive Bayes: 0.4666666666666667 0.4666666666666667\n",
      "Naive Bayes: 0.7666666666666667 0.7666666666666667\n",
      "Naive Bayes: 0.6666666666666666 0.6666666666666666\n",
      "Naive Bayes: 0.6666666666666666 0.6666666666666666\n",
      "Naive Bayes: 0.7 0.7\n",
      "Naive Bayes: 0.7241379310344828 0.7241379310344828\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10, shuffle=True, random_state=1)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    x_train = X.iloc[train_index]\n",
    "    y_train = Y[train_index]\n",
    "    \n",
    "    x_test = X.iloc[test_index]\n",
    "    y_test = Y[test_index]\n",
    "    \n",
    "    nb_predict(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2o",
   "language": "python",
   "name": "h2o"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
